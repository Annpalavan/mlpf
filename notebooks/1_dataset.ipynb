{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/cern.ch/work/m/mgarciam/private/miniconda/miniconda3/envs/graphgps/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "import os.path as osp\n",
    "import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('/afs/cern.ch/work/m/mgarciam/private/mlpf/')\n",
    "from src.dataset.dataset import SimpleIterDataset\n",
    "from src.utils.utils import to_filelist\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.data_train = ['/eos/user/m/mgarciam/datasets/pflow/tree2.root']\n",
    "        self.data_val = ['/eos/user/m/mgarciam/datasets/pflow/tree2.root']\n",
    "        #self.data_train = files_train\n",
    "        self.data_config = '/afs/cern.ch/work/m/mgarciam/private/mlpf/config_files/config_2_newlinks.yaml'\n",
    "        self.extra_selection = None\n",
    "        self.train_val_split = 0.8\n",
    "        self.data_fraction = 1\n",
    "        self.file_fraction = 1\n",
    "        self.fetch_by_files = False\n",
    "        self.fetch_step = 0.01\n",
    "        self.steps_per_epoch = None\n",
    "        self.in_memory = False\n",
    "        self.local_rank = None\n",
    "        self.copy_inputs = False\n",
    "        self.no_remake_weights = False\n",
    "        self.batch_size = 10\n",
    "        self.num_workers = 0\n",
    "        self.demo = False\n",
    "        self.laplace = False\n",
    "        self.diffs = False\n",
    "        self.class_edges = False\n",
    "args = Args()\n",
    "train_range = (0, args.train_val_split)\n",
    "train_file_dict, train_files = to_filelist(args, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SimpleIterDataset(train_file_dict, args.data_config, for_training=True,\n",
    "                                   extra_selection=args.extra_selection,\n",
    "                                   remake_weights= True,\n",
    "                                   load_range_and_fraction=(train_range, args.data_fraction),\n",
    "                                   file_fraction=args.file_fraction,\n",
    "                                   fetch_by_files=args.fetch_by_files,\n",
    "                                   fetch_step=args.fetch_step,\n",
    "                                   infinity_mode= False,\n",
    "                                   in_memory=args.in_memory,\n",
    "                                   async_load = False,\n",
    "                                   name='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Restarting DataIter train, seed=None ===\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g,gt = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=410, num_edges=168100,\n",
       "      ndata_schemes={'h': Scheme(shape=(7,), dtype=torch.float32), 'pos_hits': Scheme(shape=(3,), dtype=torch.float32), 'pos_hits_norm': Scheme(shape=(3,), dtype=torch.float32), 'hit_type': Scheme(shape=(3,), dtype=torch.int64), 'p_hits': Scheme(shape=(1,), dtype=torch.float32), 'e_hits': Scheme(shape=(1,), dtype=torch.float32), 'particle_number': Scheme(shape=(1,), dtype=torch.float32)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.ndata['particle_number'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.0332,   0.0733,   0.9968,   0.5160],\n",
       "        [-10.0000, -10.0000, -10.0000, -10.0000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Restarting DataIter train, seed=None ===\n"
     ]
    }
   ],
   "source": [
    "from src.dataset.functions_graph import graph_batch_func\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, drop_last=True, pin_memory=True,\n",
    "                          num_workers=min(args.num_workers, int(len(train_files) * args.file_fraction)),\n",
    "                          collate_fn = graph_batch_func,\n",
    "                          persistent_workers=args.num_workers > 0 and args.steps_per_epoch is not None)\n",
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g,y = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.wrapper.example_gravnet_model import GraphTransformerNetWrapper\n",
    "newmodel = GraphTransformerNetWrapper('cpu')\n",
    "newmodel.load_state_dict(torch.load('/afs/cern.ch/work/m/mgarciam/private/mlpf/models_trained/_epoch-97_state.pt', map_location='cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = newmodel(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([187, 3]) torch.Size([187, 1]) torch.Size([187, 3]) torch.Size([187, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/mgarciam/ipykernel_27928/2716966644.py:12: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  torch.range(0, len_batch - 1), g.batch_num_nodes()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "_, S = pred.shape\n",
    "xj = torch.nn.functional.normalize(pred[:, 0:3], dim=1)\n",
    "bj = torch.sigmoid(torch.reshape(pred[:, 3], [-1, 1]))\n",
    "distance_threshold = torch.sigmoid(torch.reshape(pred[:, 4:7], [-1, 3]))\n",
    "energy_correction = torch.sigmoid(torch.reshape(pred[:, 7], [-1, 1]))\n",
    "dev = g.device\n",
    "print(xj.shape, bj.shape, distance_threshold.shape, energy_correction.shape)\n",
    "clustering_index_l = g.ndata['particle_number']\n",
    "\n",
    "len_batch = len(g.batch_num_nodes())\n",
    "batch_numbers = torch.repeat_interleave(\n",
    "    torch.range(0, len_batch - 1), g.batch_num_nodes()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = bj.view(-1)\n",
    "cluster_index_per_event = clustering_index_l.view(-1).long()\n",
    "batch = batch_numbers.long()\n",
    "cluster_space_coords = xj\n",
    "qmin = 0.1\n",
    "s_B = 1.\n",
    "noise_cluster_index = 0\n",
    "beta_stabilizing = 'soft_q_scaling'\n",
    "huberize_norm_for_V_attractive = False\n",
    "beta_term_option = 'paper'\n",
    "return_components = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_max, scatter_add, scatter_mean\n",
    "from src.layers.object_cond import batch_cluster_indices, scatter_count, scatter_counts_to_indices\n",
    "cluster_index, n_clusters_per_event = batch_cluster_indices(cluster_index_per_event, batch)\n",
    "n_clusters = n_clusters_per_event.sum()\n",
    "n_hits, cluster_space_dim = cluster_space_coords.size()\n",
    "batch_size = batch.max()+1\n",
    "n_hits_per_event = scatter_count(batch)\n",
    "\n",
    "# Index of cluster -> event (n_clusters,)\n",
    "batch_cluster = scatter_counts_to_indices(n_clusters_per_event)\n",
    "\n",
    "# Per-hit boolean, indicating whether hit is sig or noise\n",
    "is_noise = cluster_index_per_event == noise_cluster_index\n",
    "is_sig = ~is_noise\n",
    "n_hits_sig = is_sig.sum()\n",
    "n_sig_hits_per_event = scatter_count(batch[is_sig])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-cluster boolean, indicating whether cluster is an object or noise\n",
    "is_object = scatter_max(is_sig.long(), cluster_index)[0].bool()\n",
    "is_noise_cluster = ~is_object\n",
    "\n",
    "# FIXME: This assumes noise_cluster_index == 0!!\n",
    "# Not sure how to do this in a performant way in case noise_cluster_index != 0\n",
    "if noise_cluster_index != 0: raise NotImplementedError\n",
    "object_index_per_event = cluster_index_per_event[is_sig] - 1\n",
    "object_index, n_objects_per_event = batch_cluster_indices(object_index_per_event, batch[is_sig])\n",
    "n_hits_per_object = scatter_count(object_index)\n",
    "batch_object = batch_cluster[is_object]\n",
    "n_objects = is_object.sum()\n",
    "\n",
    "assert object_index.size() == (n_hits_sig,)\n",
    "assert is_object.size() == (n_clusters,)\n",
    "assert torch.all(n_hits_per_object > 0)\n",
    "assert object_index.max()+1 == n_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if beta_stabilizing == 'paper':\n",
    "    q = beta.arctanh()**2 + qmin\n",
    "elif beta_stabilizing == 'clip':\n",
    "    beta = beta.clip(0., 1-1e-4)\n",
    "    q = beta.arctanh()**2 + qmin\n",
    "elif beta_stabilizing == 'soft_q_scaling':\n",
    "    q = (beta.clip(0., 1-1e-4)/1.002).arctanh()**2 + qmin\n",
    "else:\n",
    "    raise ValueError(f'beta_stablizing mode {beta_stabilizing} is not known')\n",
    "assert q.device == dev\n",
    "assert q.size() == (n_hits,)\n",
    "\n",
    "# Calculate q_alpha, the max q per object, and the indices of said maxima\n",
    "q_alpha, index_alpha = scatter_max(q[is_sig], object_index)\n",
    "assert q_alpha.size() == (n_objects,)\n",
    "\n",
    "# Get the cluster space coordinates and betas for these maxima hits too\n",
    "x_alpha = cluster_space_coords[is_sig][index_alpha]\n",
    "beta_alpha = beta[is_sig][index_alpha]\n",
    "assert x_alpha.size() == (n_objects, cluster_space_dim)\n",
    "assert beta_alpha.size() == (n_objects,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.ndata['hit_type'][index_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_particles_pred = g.ndata['pos_hits_norm'][is_sig][index_alpha]\n",
    "positions_particles_pred = positions_particles_pred+distance_threshold[is_sig][index_alpha]\n",
    "\n",
    "e_particles_pred = g.ndata['e_hits'][is_sig][index_alpha]\n",
    "e_particles_pred = e_particles_pred*energy_correction[is_sig][index_alpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_particles = y[:,0:3]\n",
    "e_particles = y[:,3].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_E = torch.mean(torch.square((e_particles_pred - e_particles)/e_particles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_E = torch.mean(torch.square((e_particles_pred - e_particles)/e_particles))\n",
    "loss_mse = torch.nn.MSELoss()\n",
    "loss_x = loss_mse(positions_particles_pred,x_particles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('graphgps': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1db199df8f75d900d458855decbcf5956490222a21736f6416c2999d256400d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
